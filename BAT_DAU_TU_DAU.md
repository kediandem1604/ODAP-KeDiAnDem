# H∆∞·ªõng D·∫´n T·ª´ ƒê·∫ßu: Extract v√† Setup D·ª± √Ån

## B∆∞·ªõc 1: Extract File ZIP

### 1.1. Di Chuy·ªÉn ƒê·∫øn Th∆∞ M·ª•c Downloads

```bash
cd ~/Downloads
```

### 1.2. Ki·ªÉm Tra File ZIP

```bash
ls -lh ODAP-KeDiAnDem-main.zip
```

### 1.3. Extract File ZIP

```bash
# Extract file zip
unzip ODAP-KeDiAnDem-main.zip

# Ho·∫∑c n·∫øu ch∆∞a c√≥ unzip, c√†i ƒë·∫∑t:
# sudo apt install unzip
# unzip ODAP-KeDiAnDem-main.zip
```

### 1.4. Ki·ªÉm Tra Th∆∞ M·ª•c ƒê√£ Extract

```bash
# Xem th∆∞ m·ª•c ƒë√£ extract
ls -la

# Di chuy·ªÉn v√†o th∆∞ m·ª•c
cd ODAP-KeDiAnDem-main

# Xem c·∫•u tr√∫c th∆∞ m·ª•c
ls -la
```

### 1.5. Di Chuy·ªÉn Th∆∞ M·ª•c ƒê·∫øn V·ªã Tr√≠ Ph√π H·ª£p (T√πy ch·ªçn)

```bash
# Di chuy·ªÉn v·ªÅ th∆∞ m·ª•c home
mv ~/Downloads/ODAP-KeDiAnDem-main ~/ODAP-22-TamBeo

# Ho·∫∑c gi·ªØ nguy√™n t√™n
cd ~/ODAP-KeDiAnDem-main
```

---

## B∆∞·ªõc 2: Ki·ªÉm Tra C·∫•u Tr√∫c D·ª± √Ån

### 2.1. Xem C·∫•u Tr√∫c Th∆∞ M·ª•c

```bash
# Trong th∆∞ m·ª•c d·ª± √°n
tree
# ho·∫∑c
ls -R
```

**C·∫•u tr√∫c mong ƒë·ª£i:**
```
ODAP-KeDiAnDem-main/
‚îú‚îÄ‚îÄ hadoop/
‚îÇ   ‚îú‚îÄ‚îÄ compact_csv.py
‚îÇ   ‚îî‚îÄ‚îÄ merge_parquet_to_csv.py
‚îú‚îÄ‚îÄ powerbi/
‚îÇ   ‚îî‚îÄ‚îÄ load_data.py
‚îú‚îÄ‚îÄ spark_streaming/
‚îÇ   ‚îú‚îÄ‚îÄ spark_credit_card_consumer.py
‚îÇ   ‚îî‚îÄ‚îÄ vietcombank_exchange_rate.py
‚îú‚îÄ‚îÄ airflow/
‚îÇ   ‚îî‚îÄ‚îÄ dags/
‚îÇ       ‚îî‚îÄ‚îÄ load_power_bi.py
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ .env (c·∫ßn t·∫°o)
```

---

## B∆∞·ªõc 3: C√†i ƒê·∫∑t Python v√† Dependencies

### 3.1. Ki·ªÉm Tra Python

```bash
# Ki·ªÉm tra Python version
python3 --version

# N·∫øu ch∆∞a c√≥, c√†i ƒë·∫∑t:
# sudo apt update
# sudo apt install python3 python3-pip
```

### 3.2. C√†i ƒê·∫∑t Th∆∞ Vi·ªán Python

**‚ö†Ô∏è L∆∞u √Ω:** N·∫øu g·∫∑p l·ªói "externally-managed-environment", d√πng m·ªôt trong c√°c c√°ch sau:

**C√°ch 1: T·∫°o Virtual Environment (Khuy·∫øn ngh·ªã)**

```bash
# Di chuy·ªÉn v√†o th∆∞ m·ª•c d·ª± √°n
cd ~/ODAP-KeDiAnDem-main
# ho·∫∑c
cd ~/ODAP-22-TamBeo

# T·∫°o virtual environment
python3 -m venv venv

# K√≠ch ho·∫°t virtual environment
source venv/bin/activate

# C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán
pip install -r requirements.txt

# Sau khi c√†i xong, lu√¥n nh·ªõ k√≠ch ho·∫°t venv tr∆∞·ªõc khi ch·∫°y script:
# source venv/bin/activate
```

**C√°ch 2: D√πng --break-system-packages (Nhanh nh∆∞ng kh√¥ng khuy·∫øn ngh·ªã)**

```bash
# Di chuy·ªÉn v√†o th∆∞ m·ª•c d·ª± √°n
cd ~/ODAP-KeDiAnDem-main

# C√†i ƒë·∫∑t v·ªõi flag --break-system-packages
pip3 install --break-system-packages -r requirements.txt
```

**C√°ch 3: D√πng --user (C√†i cho user hi·ªán t·∫°i)**

```bash
# Di chuy·ªÉn v√†o th∆∞ m·ª•c d·ª± √°n
cd ~/ODAP-KeDiAnDem-main

# C√†i ƒë·∫∑t cho user
pip3 install --user -r requirements.txt
```

### 3.3. Ki·ªÉm Tra C√°c Th∆∞ Vi·ªán ƒê√£ C√†i

```bash
# Test import c√°c th∆∞ vi·ªán
python3 -c "import pandas; import hdfs; import requests; print('OK')"
```

---

## B∆∞·ªõc 4: C·∫•u H√¨nh File .env

### 4.1. T·∫°o File .env

```bash
# Trong th∆∞ m·ª•c d·ª± √°n
nano .env
# ho·∫∑c
vim .env
```

### 4.2. Th√™m N·ªôi Dung V√†o File .env

```bash
# HDFS Configuration
HDFS_NAMENODE_URL=hdfs://localhost:9000
HDFS_WEB_HOST=localhost:9870
HDFS_USER=khtn_22120300
HDFS_COMPACTED_PATH=/credit_card_data/compacted

# Power BI Configuration
POWERBI_PUSH_URL=https://api.powerbi.com/beta/40127cd4-45f3-49a3-b05d-315a43a9f033/datasets/d0d299e5-b210-4966-a1bf-38052d6ca14a/rows?experience=power-bi&clientSideAuth=0&key=cQ%2B0ZbVDMLYHRUmWaGYPkbxVf0ZNFTVno3lWOR0DA1yBVYKVShduHno7yIe6FOwLYG01Hdp7GtsI7iGPoxQyzw%3D%3D

# File l∆∞u timestamp push cu·ªëi
LAST_PUSH_FILE=/tmp/last_push_time.txt
```

**L∆∞u √Ω:** 
- Thay `khtn_22120300` b·∫±ng username HDFS c·ªßa b·∫°n (t·ª´ h√¨nh ·∫£nh t√¥i th·∫•y b·∫°n d√πng `khtn_22120300`)
- L∆∞u file: `Ctrl+O`, Enter, `Ctrl+X` (nano) ho·∫∑c `:wq` (vim)

---

## B∆∞·ªõc 5: Ki·ªÉm Tra Hadoop HDFS

### 5.1. Ki·ªÉm Tra HDFS ƒê√£ Ch·∫°y

```bash
# Ki·ªÉm tra c√°c process
jps

# K·∫øt qu·∫£ mong ƒë·ª£i:
# - NameNode
# - DataNode
# - SecondaryNameNode
```

### 5.2. Kh·ªüi ƒê·ªông HDFS (N·∫øu Ch∆∞a Ch·∫°y)

```bash
# Di chuy·ªÉn ƒë·∫øn th∆∞ m·ª•c Hadoop
cd $HADOOP_HOME
# ho·∫∑c
cd /usr/local/hadoop

# Kh·ªüi ƒë·ªông HDFS
sbin/start-dfs.sh

# Ki·ªÉm tra l·∫°i
jps
```

### 5.3. Ki·ªÉm Tra Th∆∞ M·ª•c Trong HDFS

**‚ö†Ô∏è L∆∞u √Ω:** N·∫øu g·∫∑p l·ªói "Unknown command: dfs", xem ph·∫ßn Troubleshooting b√™n d∆∞·ªõi.

```bash
# Ki·ªÉm tra th∆∞ m·ª•c ƒë√£ t·∫°o ch∆∞a
hdfs dfs -ls /credit_card_data

# N·∫øu ch∆∞a c√≥, t·∫°o th∆∞ m·ª•c
hdfs dfs -mkdir -p /credit_card_data/parquet
hdfs dfs -mkdir -p /credit_card_data/final
hdfs dfs -mkdir -p /credit_card_data/compacted
```

**N·∫øu l·ªánh `hdfs dfs` kh√¥ng ho·∫°t ƒë·ªông, th·ª≠:**
```bash
# C√°ch 1: D√πng ƒë∆∞·ªùng d·∫´n ƒë·∫ßy ƒë·ªß
$HADOOP_HOME/bin/hdfs dfs -ls /credit_card_data

# C√°ch 2: Ki·ªÉm tra bi·∫øn m√¥i tr∆∞·ªùng
echo $HADOOP_HOME
which hdfs

# C√°ch 3: Di chuy·ªÉn ƒë·∫øn th∆∞ m·ª•c Hadoop
cd $HADOOP_HOME
bin/hdfs dfs -ls /credit_card_data
```

### 5.4. Ki·ªÉm Tra Web UI

- M·ªü tr√¨nh duy·ªát: http://localhost:9870
- Ki·ªÉm tra HDFS c√≥ ho·∫°t ƒë·ªông kh√¥ng

---

## B∆∞·ªõc 6: Ki·ªÉm Tra D·ªØ Li·ªáu trong HDFS

### 6.1. Ki·ªÉm Tra C√≥ D·ªØ Li·ªáu Parquet Kh√¥ng

```bash
# Ki·ªÉm tra th∆∞ m·ª•c Parquet
hdfs dfs -ls /credit_card_data/parquet

# N·∫øu c√≥ file, xem chi ti·∫øt
hdfs dfs -ls -h /credit_card_data/parquet
```

### 6.2. Ki·ªÉm Tra C√≥ CSV Batch Kh√¥ng

```bash
# Ki·ªÉm tra th∆∞ m·ª•c Final
hdfs dfs -ls /credit_card_data/final

# Xem n·ªôi dung file (n·∫øu c√≥)
hdfs dfs -cat /credit_card_data/final/batch_*.csv | head -20
```

### 6.3. Ki·ªÉm Tra File Compacted

```bash
# Ki·ªÉm tra file compacted
hdfs dfs -ls /credit_card_data/compacted

# N·∫øu kh√¥ng c√≥ file, c·∫ßn ch·∫°y compact_csv.py (xem B∆∞·ªõc 8)
```

---

## B∆∞·ªõc 7: Test Script Push D·ªØ Li·ªáu

### 7.1. Ki·ªÉm Tra File Script

```bash
# Trong th∆∞ m·ª•c d·ª± √°n
cd ~/ODAP-KeDiAnDem-main
# ho·∫∑c
cd ~/ODAP-22-TamBeo

# Ki·ªÉm tra script c√≥ t·ªìn t·∫°i kh√¥ng
ls -la powerbi/load_data.py
```

### 7.2. Test K·∫øt N·ªëi HDFS

```bash
# Test k·∫øt n·ªëi HDFS t·ª´ Python
python3 -c "from hdfs import InsecureClient; client = InsecureClient('http://localhost:9870', user='khtn_22120300'); print(client.list('/credit_card_data'))"
```

### 7.3. Ch·∫°y Script Push D·ªØ Li·ªáu

```bash
# N·∫øu d√πng virtual environment, k√≠ch ho·∫°t tr∆∞·ªõc:
source venv/bin/activate

# Ch·∫°y script
python3 powerbi/load_data.py
# ho·∫∑c n·∫øu d√πng venv:
python powerbi/load_data.py
```

**‚ö†Ô∏è L∆∞u √Ω:** N·∫øu g·∫∑p l·ªói "Kh√¥ng t√¨m th·∫•y file CSV compacted", c·∫ßn ch·∫°y `compact_csv.py` tr∆∞·ªõc (xem B∆∞·ªõc 8).

**K·∫øt qu·∫£ mong ƒë·ª£i:**
- Script ƒë·ªçc file CSV t·ª´ HDFS
- L·ªçc d·ªØ li·ªáu m·ªõi
- T√≠nh to√°n c√°c c·ªôt: TxnDate, Hour, TimeBucket60Min, TimeBucket2H, DayOfWeekNum, DayOfWeekName, IsWeekend, HasErrorFlag, IsFraudFlag
- Push d·ªØ li·ªáu l√™n Power BI
- Hi·ªÉn th·ªã s·ªë d√≤ng ƒë√£ push

---

## B∆∞·ªõc 8: T·∫°o File CSV Compacted (QUAN TR·ªåNG!)

### 8.1. Ki·ªÉm Tra C√≥ D·ªØ Li·ªáu Trong HDFS

```bash
# Ki·ªÉm tra c√≥ CSV batch trong /credit_card_data/final kh√¥ng
hdfs dfs -ls /credit_card_data/final

# Ki·ªÉm tra c√≥ Parquet trong /credit_card_data/parquet kh√¥ng
hdfs dfs -ls /credit_card_data/parquet
```

### 8.2. N·∫øu C√≥ CSV Batch - G·ªôp Th√†nh Compacted

**N·∫øu th·∫•y file CSV batch (batch_*.csv) trong `/credit_card_data/final`:**

```bash
# Trong th∆∞ m·ª•c d·ª± √°n
cd ~/Downloads/ODAP-KeDiAnDem-main

# K√≠ch ho·∫°t venv (n·∫øu d√πng)
source venv/bin/activate

# Ch·∫°y script g·ªôp CSV batch th√†nh compacted
spark-submit hadoop/compact_csv.py
```

**Ki·ªÉm tra k·∫øt qu·∫£:**
```bash
# Ki·ªÉm tra file compacted ƒë√£ ƒë∆∞·ª£c t·∫°o
hdfs dfs -ls /credit_card_data/compacted
```

### 8.3. N·∫øu C√≥ Parquet - G·ªôp Th√†nh CSV Batch Tr∆∞·ªõc

**N·∫øu ch·ªâ c√≥ Parquet, c·∫ßn g·ªôp th√†nh CSV batch tr∆∞·ªõc:**

```bash
# Trong th∆∞ m·ª•c d·ª± √°n
cd ~/Downloads/ODAP-KeDiAnDem-main

# K√≠ch ho·∫°t venv (n·∫øu d√πng)
source venv/bin/activate

# B∆∞·ªõc 1: G·ªôp Parquet th√†nh CSV batch
spark-submit hadoop/merge_parquet_to_csv.py

# Ki·ªÉm tra k·∫øt qu·∫£
hdfs dfs -ls /credit_card_data/final

# B∆∞·ªõc 2: G·ªôp CSV batch th√†nh compacted
spark-submit hadoop/compact_csv.py

# Ki·ªÉm tra k·∫øt qu·∫£
hdfs dfs -ls /credit_card_data/compacted
```

### 8.4. N·∫øu Kh√¥ng C√≥ D·ªØ Li·ªáu - C·∫ßn T·∫°o D·ªØ Li·ªáu

**N·∫øu kh√¥ng c√≥ d·ªØ li·ªáu trong HDFS, b·∫°n c·∫ßn:**

1. **Ch·∫°y Kafka Producer** ƒë·ªÉ t·∫°o d·ªØ li·ªáu
2. **Ch·∫°y Spark Streaming Consumer** ƒë·ªÉ x·ª≠ l√Ω v√† l∆∞u v√†o HDFS
3. **Sau ƒë√≥ m·ªõi ch·∫°y c√°c script g·ªôp**

Xem h∆∞·ªõng d·∫´n trong `README.md` ƒë·ªÉ ch·∫°y to√†n b·ªô pipeline.

---

## B∆∞·ªõc 9: Ch·∫°y T·ª± ƒê·ªông qua Airflow (Khuy·∫øn ngh·ªã)

### 9.1. Ki·ªÉm Tra Airflow

```bash
# Ki·ªÉm tra Airflow
airflow version
```

### 9.2. Kh·ªüi ƒê·ªông Airflow Scheduler

**Terminal 1:**

```bash
cd ~/ODAP-KeDiAnDem-main
export AIRFLOW_HOME=$(pwd)/airflow
airflow scheduler
```

### 9.3. Kh·ªüi ƒê·ªông Airflow Webserver

**Terminal 2 (Terminal m·ªõi):**

```bash
cd ~/ODAP-KeDiAnDem-main
export AIRFLOW_HOME=$(pwd)/airflow
airflow webserver --port 8080
```

### 9.4. Truy C·∫≠p Airflow Web UI

1. **M·ªü tr√¨nh duy·ªát:** http://localhost:8080
2. **ƒêƒÉng nh·∫≠p** (n·∫øu c·∫ßn):
   - Username: `admin`
   - Password: `admin` (ho·∫∑c password b·∫°n ƒë√£ set)

### 9.5. K√≠ch Ho·∫°t DAG

1. **T√¨m DAG:** `send_data_to_powerbi`
2. **Toggle ON** ƒë·ªÉ k√≠ch ho·∫°t
3. **DAG s·∫Ω t·ª± ƒë·ªông ch·∫°y m·ªói 10 ph√∫t**

---

## B∆∞·ªõc 10: Ki·ªÉm Tra K·∫øt Qu·∫£

### 10.1. Ki·ªÉm Tra Power BI

1. **V√†o Power BI Web:** https://app.powerbi.com
2. **V√†o dataset "RealTimeData"**
3. **Xem tab "Data"** ‚Üí C√≥ d·ªØ li·ªáu ch∆∞a?
4. **V√†o dashboard** ‚Üí Visualizations c√≥ c·∫≠p nh·∫≠t kh√¥ng?

### 10.2. Ki·ªÉm Tra Log Script

```bash
# Xem log khi ch·∫°y script
python3 powerbi/load_data.py

# Ki·ªÉm tra:
# - C√≥ ƒë·ªçc ƒë∆∞·ª£c file t·ª´ HDFS kh√¥ng?
# - C√≥ push ƒë∆∞·ª£c d·ªØ li·ªáu l√™n Power BI kh√¥ng?
# - C√≥ l·ªói g√¨ kh√¥ng?
```

---

## Troubleshooting

### L·ªói: "unzip: command not found"

```bash
sudo apt update
sudo apt install unzip
```

### L·ªói: "externally-managed-environment"

**Gi·∫£i ph√°p:** D√πng virtual environment (C√°ch 1 ·ªü tr√™n) ho·∫∑c `--break-system-packages`

### L·ªói: "Module not found" khi ch·∫°y Python

```bash
# N·∫øu d√πng virtual environment, ƒë·∫£m b·∫£o ƒë√£ k√≠ch ho·∫°t:
source venv/bin/activate

# C√†i ƒë·∫∑t l·∫°i th∆∞ vi·ªán
pip install -r requirements.txt

# Ho·∫∑c c√†i t·ª´ng c√°i
pip install pandas hdfs requests python-dotenv
```

### L·ªói: "Connection refused" khi k·∫øt n·ªëi HDFS

- Ki·ªÉm tra HDFS ƒë√£ kh·ªüi ƒë·ªông: `jps`
- Kh·ªüi ƒë·ªông HDFS: `start-dfs.sh`
- Ki·ªÉm tra file `.env` c√≥ c·∫•u h√¨nh ƒë√∫ng kh√¥ng

### L·ªói: "Unknown command: dfs"

**Nguy√™n nh√¢n:** Hadoop ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t ƒë√∫ng ho·∫∑c bi·∫øn m√¥i tr∆∞·ªùng ch∆∞a ƒë∆∞·ª£c c·∫•u h√¨nh.

**Gi·∫£i ph√°p:**

1. **Ki·ªÉm tra Hadoop ƒë√£ c√†i ch∆∞a:**
```bash
# Ki·ªÉm tra bi·∫øn m√¥i tr∆∞·ªùng
echo $HADOOP_HOME
echo $JAVA_HOME

# Ki·ªÉm tra file hdfs c√≥ t·ªìn t·∫°i kh√¥ng
which hdfs
ls -la $HADOOP_HOME/bin/hdfs
```

2. **N·∫øu ch∆∞a c√≥, c√†i ƒë·∫∑t ho·∫∑c c·∫•u h√¨nh:**
```bash
# C·∫•u h√¨nh bi·∫øn m√¥i tr∆∞·ªùng trong ~/.bashrc
export HADOOP_HOME=/usr/local/hadoop
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64

# Reload
source ~/.bashrc
```

3. **D√πng ƒë∆∞·ªùng d·∫´n ƒë·∫ßy ƒë·ªß:**
```bash
# Thay v√¨: hdfs dfs -ls
# D√πng:
$HADOOP_HOME/bin/hdfs dfs -ls /credit_card_data

# Ho·∫∑c:
/usr/local/hadoop/bin/hdfs dfs -ls /credit_card_data
```

4. **Ki·ªÉm tra Hadoop ƒë√£ kh·ªüi ƒë·ªông ch∆∞a:**
```bash
jps
# Ph·∫£i th·∫•y: NameNode, DataNode, SecondaryNameNode
```

### L·ªói: "Permission denied" tr√™n HDFS

```bash
# C·∫•p quy·ªÅn cho th∆∞ m·ª•c
hdfs dfs -chmod -R 777 /credit_card_data

# Ho·∫∑c ƒë·ªïi owner
hdfs dfs -chown -R khtn_22120300:supergroup /credit_card_data

# N·∫øu l·ªánh hdfs dfs kh√¥ng ho·∫°t ƒë·ªông, d√πng:
$HADOOP_HOME/bin/hdfs dfs -chmod -R 777 /credit_card_data
```

### L·ªói: "401 Unauthorized" khi push data

- Ki·ªÉm tra Push URL c√≥ ƒë√∫ng kh√¥ng
- Ki·ªÉm tra key trong URL c√≥ ƒë·∫ßy ƒë·ªß kh√¥ng
- C√≥ th·ªÉ URL ƒë√£ h·∫øt h·∫°n, c·∫ßn l·∫•y l·∫°i t·ª´ Power BI

---

## Checklist Ho√†n Th√†nh

- [ ] ƒê√£ extract file ZIP
- [ ] ƒê√£ c√†i ƒë·∫∑t Python v√† dependencies
- [ ] ƒê√£ t·∫°o file `.env` v·ªõi c·∫•u h√¨nh ƒë√∫ng
- [ ] ƒê√£ kh·ªüi ƒë·ªông HDFS
- [ ] ƒê√£ t·∫°o th∆∞ m·ª•c trong HDFS
- [ ] ƒê√£ test script `load_data.py`
- [ ] ƒê√£ ki·ªÉm tra d·ªØ li·ªáu tr√™n Power BI
- [ ] ƒê√£ setup Airflow (n·∫øu c·∫ßn)

---

## T√≥m T·∫Øt L·ªánh Nhanh

```bash
# 1. Extract
cd ~/Downloads
unzip ODAP-KeDiAnDem-main.zip
cd ODAP-KeDiAnDem-main

# 2. T·∫°o virtual environment v√† c√†i ƒë·∫∑t dependencies
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt

# 3. T·∫°o file .env
nano .env
# (Paste n·ªôi dung t·ª´ B∆∞·ªõc 4.2)

# 4. Kh·ªüi ƒë·ªông HDFS
cd $HADOOP_HOME
start-dfs.sh

# 5. Test script (nh·ªõ k√≠ch ho·∫°t venv tr∆∞·ªõc)
cd ~/ODAP-KeDiAnDem-main
source venv/bin/activate
python powerbi/load_data.py
```

---

Ch√∫c b·∫°n th√†nh c√¥ng! üéâ

