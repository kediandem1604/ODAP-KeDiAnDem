# H∆∞·ªõng D·∫´n Ch·∫°y Hadoop tr√™n VMware v√† Push D·ªØ Li·ªáu l√™n Power BI

## ‚úÖ Th√¥ng Tin API Power BI

**Push URL:** 
```
https://api.powerbi.com/beta/40127cd4-45f3-49a3-b05d-315a43a9f033/datasets/d0d299e5-b210-4966-a1bf-38052d6ca14a/rows?experience=power-bi&clientSideAuth=0&key=cQ%2B0ZbVDMLYHRUmWaGYPkbxVf0ZNFTVno3lWOR0DA1yBVYKVShduHno7yIe6FOwLYG01Hdp7GtsI7iGPoxQyzw%3D%3D
```

---

## B∆∞·ªõc 1: C·∫•u H√¨nh File .env

### 1.1. M·ªü ho·∫∑c t·∫°o file `.env`

```bash
cd ODAP-22-TamBeo
nano .env
# ho·∫∑c
vim .env
# ho·∫∑c d√πng editor b·∫•t k·ª≥
```

### 1.2. Th√™m/ C·∫≠p nh·∫≠t c√°c d√≤ng sau:

```bash
# Power BI Configuration
POWERBI_PUSH_URL=https://api.powerbi.com/beta/40127cd4-45f3-49a3-b05d-315a43a9f033/datasets/d0d299e5-b210-4966-a1bf-38052d6ca14a/rows?experience=power-bi&clientSideAuth=0&key=cQ%2B0ZbVDMLYHRUmWaGYPkbxVf0ZNFTVno3lWOR0DA1yBVYKVShduHno7yIe6FOwLYG01Hdp7GtsI7iGPoxQyzw%3D%3D

# HDFS Configuration
HDFS_NAMENODE_URL=hdfs://localhost:9000
HDFS_WEB_HOST=localhost:9870
HDFS_USER=panda
HDFS_COMPACTED_PATH=/credit_card_data/compacted

# File l∆∞u timestamp push cu·ªëi
LAST_PUSH_FILE=/tmp/last_push_time.txt
```

### 1.3. L∆∞u file

---

## B∆∞·ªõc 2: Kh·ªüi ƒê·ªông Hadoop tr√™n VMware

### 2.1. M·ªü Terminal trong VMware

- M·ªü terminal trong m√°y ·∫£o Linux (Ubuntu/CentOS)
- Ho·∫∑c SSH v√†o m√°y ·∫£o t·ª´ m√°y host

### 2.2. Ki·ªÉm Tra Hadoop ƒê√£ C√†i ƒê·∫∑t

```bash
# Ki·ªÉm tra Hadoop version
hadoop version

# Ki·ªÉm tra bi·∫øn m√¥i tr∆∞·ªùng
echo $HADOOP_HOME
echo $JAVA_HOME
```

**N·∫øu ch∆∞a c√†i ƒë·∫∑t Hadoop:**
- Xem h∆∞·ªõng d·∫´n c√†i ƒë·∫∑t Hadoop ·ªü cu·ªëi file n√†y

### 2.3. Kh·ªüi ƒê·ªông HDFS

```bash
# Di chuy·ªÉn ƒë·∫øn th∆∞ m·ª•c Hadoop
cd $HADOOP_HOME

# Kh·ªüi ƒë·ªông HDFS
sbin/start-dfs.sh

# Ho·∫∑c n·∫øu ƒë√£ c√≥ trong PATH:
start-dfs.sh
```

### 2.4. Ki·ªÉm Tra HDFS ƒê√£ Ch·∫°y

```bash
# Ki·ªÉm tra c√°c process
jps

# K·∫øt qu·∫£ mong ƒë·ª£i:
# - NameNode
# - DataNode  
# - SecondaryNameNode
```

### 2.5. Ki·ªÉm Tra Web UI

- **NameNode Web UI:** http://localhost:9870
- M·ªü tr√¨nh duy·ªát v√† truy c·∫≠p ƒë·ªÉ xem HDFS ƒëang ch·∫°y

### 2.6. Ki·ªÉm Tra K·∫øt N·ªëi HDFS

```bash
# Ki·ªÉm tra HDFS c√≥ ho·∫°t ƒë·ªông kh√¥ng
hdfs dfsadmin -report

# Ki·ªÉm tra th∆∞ m·ª•c g·ªëc
hdfs dfs -ls /

# T·∫°o th∆∞ m·ª•c test (n·∫øu c·∫ßn)
hdfs dfs -mkdir -p /credit_card_data/parquet
hdfs dfs -mkdir -p /credit_card_data/final
hdfs dfs -mkdir -p /credit_card_data/compacted
```

---

## B∆∞·ªõc 3: Ki·ªÉm Tra D·ªØ Li·ªáu trong HDFS

### 3.1. Ki·ªÉm Tra C√≥ D·ªØ Li·ªáu Parquet Kh√¥ng

```bash
# Ki·ªÉm tra th∆∞ m·ª•c Parquet
hdfs dfs -ls /credit_card_data/parquet

# N·∫øu c√≥ file, xem chi ti·∫øt
hdfs dfs -ls -h /credit_card_data/parquet
```

### 3.2. Ki·ªÉm Tra C√≥ CSV Batch Kh√¥ng

```bash
# Ki·ªÉm tra th∆∞ m·ª•c Final (CSV batch)
hdfs dfs -ls /credit_card_data/final

# Xem n·ªôi dung file (n·∫øu c√≥)
hdfs dfs -cat /credit_card_data/final/batch_*.csv | head -20
```

### 3.3. Ki·ªÉm Tra File Compacted

```bash
# Ki·ªÉm tra file compacted
hdfs dfs -ls /credit_card_data/compacted
```

---

## B∆∞·ªõc 4: Ch·∫°y Pipeline X·ª≠ L√Ω D·ªØ Li·ªáu

### 4.1. G·ªôp Parquet th√†nh CSV Batch (N·∫øu C√≥ D·ªØ Li·ªáu Parquet M·ªõi)

```bash
cd ODAP-22-TamBeo

# Ch·∫°y script g·ªôp Parquet th√†nh CSV batch
spark-submit hadoop/merge_parquet_to_csv.py
```

**K·∫øt qu·∫£:**
- File CSV batch s·∫Ω ƒë∆∞·ª£c t·∫°o trong `/credit_card_data/final`
- T√™n file: `batch_YYYYMMDD_HHMMSS.csv`

### 4.2. G·ªôp CSV Batch th√†nh File Compacted

```bash
cd ODAP-22-TamBeo

# Ch·∫°y script g·ªôp c√°c batch CSV th√†nh 1 file compacted
spark-submit hadoop/compact_csv.py
```

**K·∫øt qu·∫£:**
- File CSV compacted s·∫Ω ƒë∆∞·ª£c t·∫°o trong `/credit_card_data/compacted`
- T√™n file: `part-00000-*.csv`

### 4.3. Push D·ªØ Li·ªáu l√™n Power BI

```bash
cd ODAP-22-TamBeo

# Ch·∫°y script push d·ªØ li·ªáu l√™n Power BI
python3 powerbi/load_data.py
```

**K·∫øt qu·∫£:**
- Script ƒë·ªçc file CSV compacted t·ª´ HDFS
- L·ªçc d·ªØ li·ªáu m·ªõi (d·ª±a tr√™n timestamp)
- T√≠nh to√°n c√°c c·ªôt: TxnDate, Hour, TimeBucket60Min, TimeBucket2H, DayOfWeekNum, DayOfWeekName, IsWeekend, HasErrorFlag, IsFraudFlag
- Push d·ªØ li·ªáu l√™n Power BI qua API
- Dashboard t·ª± ƒë·ªông c·∫≠p nh·∫≠t

---

## B∆∞·ªõc 5: Ch·∫°y T·ª± ƒê·ªông qua Airflow (Khuy·∫øn ngh·ªã)

### 5.1. Kh·ªüi ƒê·ªông Airflow Scheduler

**Terminal 1:**

```bash
cd ODAP-22-TamBeo
export AIRFLOW_HOME=$(pwd)/airflow
airflow scheduler
```

**ƒê·ªÉ terminal n√†y ch·∫°y (kh√¥ng ƒë√≥ng)**

### 5.2. Kh·ªüi ƒê·ªông Airflow Webserver

**Terminal 2 (Terminal m·ªõi):**

```bash
cd ODAP-22-TamBeo
export AIRFLOW_HOME=$(pwd)/airflow
airflow webserver --port 8080
```

**ƒê·ªÉ terminal n√†y ch·∫°y (kh√¥ng ƒë√≥ng)**

### 5.3. Truy C·∫≠p Airflow Web UI

1. **M·ªü tr√¨nh duy·ªát:** http://localhost:8080
2. **ƒêƒÉng nh·∫≠p** (n·∫øu c·∫ßn):
   - Username: `admin`
   - Password: `admin` (ho·∫∑c password b·∫°n ƒë√£ set)

### 5.4. K√≠ch Ho·∫°t DAG

1. **T√¨m DAG:** `send_data_to_powerbi`
2. **Toggle ON** (n√∫t b·∫≠t/t·∫Øt ·ªü b√™n tr√°i t√™n DAG) ƒë·ªÉ k√≠ch ho·∫°t
3. **DAG s·∫Ω t·ª± ƒë·ªông:**
   - Ch·∫°y `compact_csv.py` m·ªói 10 ph√∫t
   - Ch·∫°y `load_data.py` ƒë·ªÉ push l√™n Power BI
   - Dashboard t·ª± ƒë·ªông c·∫≠p nh·∫≠t

---

## B∆∞·ªõc 6: Ki·ªÉm Tra v√† Monitor

### 6.1. Ki·ªÉm Tra Airflow

- V√†o http://localhost:8080
- Xem DAG c√≥ ch·∫°y ƒë·ªÅu kh√¥ng (m·ªói 10 ph√∫t)
- Xem log n·∫øu c√≥ l·ªói

### 6.2. Ki·ªÉm Tra Power BI

1. **V√†o Power BI Web:** https://app.powerbi.com
2. **V√†o dataset "RealTimeData"**
3. **Xem tab "Data"** ‚Üí C√≥ d·ªØ li·ªáu ch∆∞a?
4. **V√†o dashboard** ‚Üí Visualizations c√≥ c·∫≠p nh·∫≠t kh√¥ng?

### 6.3. Ki·ªÉm Tra HDFS

```bash
# Xem t·∫•t c·∫£ d·ªØ li·ªáu trong HDFS
hdfs dfs -ls -R /credit_card_data

# Xem n·ªôi dung file CSV (10 d√≤ng ƒë·∫ßu)
hdfs dfs -cat /credit_card_data/compacted/part-*.csv | head -10

# ƒê·∫øm s·ªë d√≤ng trong file
hdfs dfs -cat /credit_card_data/compacted/part-*.csv | wc -l
```

---

## Troubleshooting

### L·ªói: "NameNode is not formatted"

```bash
# Format NameNode (CH·ªà ch·∫°y l·∫ßn ƒë·∫ßu ho·∫∑c khi reset)
hdfs namenode -format
```

‚ö†Ô∏è **C·∫£nh b√°o:** Format s·∫Ω x√≥a to√†n b·ªô d·ªØ li·ªáu trong HDFS!

### L·ªói: "Connection refused" khi k·∫øt n·ªëi HDFS

- Ki·ªÉm tra HDFS ƒë√£ kh·ªüi ƒë·ªông ch∆∞a: `jps`
- Ki·ªÉm tra port: NameNode th∆∞·ªùng ·ªü port 9000 ho·∫∑c 9870
- Ki·ªÉm tra firewall: `sudo ufw status`
- Ki·ªÉm tra file `.env` c√≥ c·∫•u h√¨nh ƒë√∫ng `HDFS_NAMENODE_URL` kh√¥ng

### L·ªói: "Permission denied" tr√™n HDFS

```bash
# C·∫•p quy·ªÅn cho th∆∞ m·ª•c
hdfs dfs -chmod -R 777 /credit_card_data

# Ho·∫∑c ƒë·ªïi owner
hdfs dfs -chown -R panda:panda /credit_card_data
```

### L·ªói: "Kh√¥ng t√¨m th·∫•y file CSV compacted"

- Ch·∫°y `spark-submit hadoop/compact_csv.py` tr∆∞·ªõc
- Ki·ªÉm tra c√≥ d·ªØ li·ªáu trong `/credit_card_data/final` kh√¥ng
- Ki·ªÉm tra script c√≥ ch·∫°y th√†nh c√¥ng kh√¥ng

### L·ªói: "401 Unauthorized" khi push data

- Ki·ªÉm tra Push URL c√≥ ƒë√∫ng kh√¥ng
- Ki·ªÉm tra key trong URL c√≥ ƒë·∫ßy ƒë·ªß kh√¥ng
- C√≥ th·ªÉ URL ƒë√£ h·∫øt h·∫°n, c·∫ßn l·∫•y l·∫°i t·ª´ Power BI

### L·ªói: "Module not found" khi ch·∫°y Python

```bash
# C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt
pip install -r requirements.txt

# Ho·∫∑c c√†i t·ª´ng c√°i
pip install pandas hdfs requests python-dotenv
```

### DAG kh√¥ng ch·∫°y trong Airflow

- Ki·ªÉm tra Airflow scheduler c√≥ ch·∫°y kh√¥ng (Terminal 1)
- Ki·ªÉm tra DAG c√≥ ƒë∆∞·ª£c toggle ON ch∆∞a
- Xem log ƒë·ªÉ t√¨m l·ªói c·ª• th·ªÉ: Click v√†o DAG ‚Üí Task ‚Üí Log

---

## C√†i ƒê·∫∑t Hadoop tr√™n VMware (N·∫øu Ch∆∞a C√≥)

### Y√™u C·∫ßu:
- Java JDK 8 ho·∫∑c 11
- SSH ƒë√£ c·∫•u h√¨nh
- T√†i kho·∫£n user c√≥ quy·ªÅn sudo

### C√°c B∆∞·ªõc:

1. **C√†i ƒë·∫∑t Java:**
```bash
sudo apt update
sudo apt install openjdk-8-jdk
java -version
```

2. **T·∫£i Hadoop:**
```bash
cd ~
wget https://archive.apache.org/dist/hadoop/common/hadoop-3.3.4/hadoop-3.3.4.tar.gz
tar -xzf hadoop-3.3.4.tar.gz
sudo mv hadoop-3.3.4 /opt/hadoop
```

3. **C·∫•u h√¨nh Environment Variables:**
```bash
# Th√™m v√†o ~/.bashrc
export HADOOP_HOME=/opt/hadoop
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop

# Reload
source ~/.bashrc
```

4. **C·∫•u h√¨nh Hadoop:**
- Ch·ªânh s·ª≠a c√°c file trong `$HADOOP_HOME/etc/hadoop/`
- Xem h∆∞·ªõng d·∫´n chi ti·∫øt: https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html

5. **Format v√† Kh·ªüi ƒê·ªông:**
```bash
hdfs namenode -format
start-dfs.sh
```

---

## T√≥m T·∫Øt L·ªánh Quan Tr·ªçng

```bash
# 1. Kh·ªüi ƒë·ªông HDFS
start-dfs.sh

# 2. Ki·ªÉm tra HDFS
jps
hdfs dfs -ls /credit_card_data

# 3. G·ªôp Parquet ‚Üí CSV batch
spark-submit hadoop/merge_parquet_to_csv.py

# 4. G·ªôp CSV batch ‚Üí Compacted
spark-submit hadoop/compact_csv.py

# 5. Push l√™n Power BI
python3 powerbi/load_data.py

# 6. Ch·∫°y Airflow (2 terminal)
# Terminal 1:
export AIRFLOW_HOME=$(pwd)/airflow
airflow scheduler

# Terminal 2:
export AIRFLOW_HOME=$(pwd)/airflow
airflow webserver --port 8080
```

---

## K·∫øt Qu·∫£ Mong ƒê·ª£i

‚úÖ HDFS ch·∫°y v√† l∆∞u tr·ªØ d·ªØ li·ªáu Parquet  
‚úÖ Script t·ª± ƒë·ªông g·ªôp Parquet ‚Üí CSV batch ‚Üí Compacted  
‚úÖ Script t·ª± ƒë·ªông push d·ªØ li·ªáu t·ª´ HDFS l√™n Power BI  
‚úÖ Dashboard Power BI t·ª± ƒë·ªông c·∫≠p nh·∫≠t m·ªói 10 ph√∫t  
‚úÖ Kh√¥ng c·∫ßn can thi·ªáp th·ªß c√¥ng  

Ch√∫c b·∫°n th√†nh c√¥ng! üéâ

