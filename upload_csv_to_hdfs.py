#!/usr/bin/env python3
"""
Script upload CSV t·ª´ dataset/ v√†o HDFS
Upload file CSV v√†o /credit_card_data/final ƒë·ªÉ sau ƒë√≥ c√≥ th·ªÉ ch·∫°y compact_csv.py
"""

import os
from hdfs import InsecureClient
from dotenv import load_dotenv, find_dotenv
from datetime import datetime

load_dotenv(find_dotenv())

HDFS_HOST = os.getenv("HDFS_WEB_HOST", "localhost:9870")
HDFS_USER = os.getenv("HDFS_USER", "khtn_22120300")
HDFS_FINAL_PATH = os.getenv("HDFS_FINAL_OUTPUT_PATH", "/credit_card_data/final")

# Kh·ªüi t·∫°o HDFS client
print("ƒêang k·∫øt n·ªëi v·ªõi HDFS...")
client = InsecureClient(f'http://{HDFS_HOST}', user=HDFS_USER)

# ƒê∆∞·ªùng d·∫´n file CSV
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
csv_file = os.path.join(BASE_DIR, "dataset", "User0_credit_card_transactions.csv")

# Ki·ªÉm tra file c√≥ t·ªìn t·∫°i kh√¥ng
if not os.path.exists(csv_file):
    print(f"‚ùå Kh√¥ng t√¨m th·∫•y file: {csv_file}")
    print(f"   Ki·ªÉm tra file c√≥ trong th∆∞ m·ª•c dataset/ kh√¥ng")
    exit(1)

# T·∫°o t√™n file v·ªõi timestamp
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
hdfs_filename = f"batch_manual_{timestamp}.csv"
hdfs_path = f"{HDFS_FINAL_PATH}/{hdfs_filename}"

# Upload file
print(f"\nƒêang upload {csv_file} v√†o HDFS...")
print(f"ƒê√≠ch: {hdfs_path}")

try:
    with open(csv_file, 'rb') as f:
        client.write(hdfs_path, f, overwrite=True)
    
    print(f"‚úÖ ƒê√£ upload th√†nh c√¥ng!")
    print(f"   File: {hdfs_path}")
    print(f"\nüìã B∆∞·ªõc ti·∫øp theo:")
    print(f"   1. Ch·∫°y: spark-submit hadoop/compact_csv.py")
    print(f"   2. Sau ƒë√≥ ch·∫°y: python3 powerbi/load_data.py")
    
except Exception as e:
    print(f"‚ùå L·ªói khi upload: {e}")
    import traceback
    traceback.print_exc()
    exit(1)

